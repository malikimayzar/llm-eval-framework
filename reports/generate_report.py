"""
reports/generate_report.py
──────────────────────────
Generate HTML evaluation report dari hasil eval yang sudah ada.

Usage:
    python reports/generate_report.py
    python reports/generate_report.py --output reports/report.html
    python reports/generate_report.py --embed-charts   # chart jadi base64 inline
"""

import json
import base64
import argparse
from datetime import datetime, timezone
from pathlib import Path

from jinja2 import Environment, FileSystemLoader

# ── Paths ────────────────────────────────────────────────────────────────────

REPORTS_DIR   = Path(__file__).parent
RESULTS_DIR   = REPORTS_DIR / "results"
CHARTS_DIR    = REPORTS_DIR / "charts"
TEMPLATES_DIR = REPORTS_DIR / "templates"


# ── Data loaders ──────────────────────────────────────────────────────────────

def load_latest_faithfulness(model: str, dataset: str) -> list:
    safe = model.replace(":", "-")
    files = sorted(RESULTS_DIR.glob(f"{safe}_{dataset}_*_faithfulness.json"))
    if not files:
        return []
    return json.loads(files[-1].read_text())


def load_latest_summary(model: str, dataset: str) -> dict:
    safe = model.replace(":", "-")
    files = sorted(RESULTS_DIR.glob(f"{safe}_{dataset}_*_summary.json"))
    if not files:
        return {}
    return json.loads(files[-1].read_text())


# ── Fallback data (dari analysis.md) — dipakai jika hasil JSON belum ada ─────

MISTRAL_CLEAN_DATA = [
    {"id": "clean_001", "score": 0.0,   "question": "What are the default values of skip and limit?",              "note": "FALSE INSUFFICIENT_CONTEXT (sim=0.7249)"},
    {"id": "clean_002", "score": 1.0,   "question": "What happens to query parameter values declared with Python types?", "note": "Perfect"},
    {"id": "clean_003", "score": 1.0,   "question": "If item_id is declared as int, what does FastAPI do?",         "note": "Perfect"},
    {"id": "clean_004", "score": 0.0,   "question": "Does FastAPI require path and query params to be declared?",   "note": "FALSE INSUFFICIENT_CONTEXT (sim=0.9691)"},
    {"id": "clean_005", "score": 1.0,   "question": "How do you make a query parameter required in FastAPI?",       "note": "Perfect"},
    {"id": "clean_006", "score": 0.667, "question": "What are the three effects of adding max_length=50?",          "note": "Hallucination: added OpenAPI schema info"},
    {"id": "clean_007", "score": 1.0,   "question": "What does the OpenAPI schema generated by FastAPI include?",   "note": "Perfect"},
    {"id": "clean_008", "score": 0.0,   "question": "URL path for /home/johndoe/myfile.txt — how to declare?",     "note": "Short answer blind spot (fixed in v1.2.1)"},
    {"id": "clean_009", "score": 0.0,   "question": "What HTTP method is used to fully replace a resource?",       "note": "Short answer blind spot (fixed in v1.2.1)"},
    {"id": "clean_010", "score": 1.0,   "question": "What happens when FastAPI receives a single Pydantic model?", "note": "Perfect"},
]

MISTRAL_DISTRACTOR_DATA = [
    {"id": "distractor_001", "score": 0.0,  "question": "What are the default values of skip and limit?",     "note": "Followed manipulated values (skip=5, limit=100)"},
    {"id": "distractor_002", "score": 0.0,  "question": "What HTTP method creates a resource?",               "note": "Followed flipped HTTP method"},
    {"id": "distractor_003", "score": None, "question": "N/A",                                                 "note": "Inference failed"},
    {"id": "distractor_004", "score": 0.0,  "question": "How to declare a path parameter for file paths?",    "note": "Short answer: ':filepath'"},
    {"id": "distractor_005", "score": 0.333,"question": "What are the effects of adding a Query constraint?", "note": "3 claims, 1 hallucination (added 'caching')"},
]

PHI3_CLEAN_DATA = [
    {"id": "clean_001", "score": 0.0,  "note": "FALSE INSUFFICIENT_CONTEXT"},
    {"id": "clean_002", "score": 1.0,  "note": "Perfect"},
    {"id": "clean_003", "score": 1.0,  "note": "Perfect"},
    {"id": "clean_004", "score": 0.0,  "note": "FALSE INSUFFICIENT_CONTEXT"},
    {"id": "clean_005", "score": 1.0,  "note": "Perfect"},
    {"id": "clean_006", "score": 0.4,  "note": "Partial — verbose but imprecise"},
    {"id": "clean_007", "score": 0.0,  "note": "7 claims generated, all below threshold"},
    {"id": "clean_008", "score": 1.0,  "note": "Perfect"},
    {"id": "clean_009", "score": 0.0,  "note": "Wrong HTTP method"},
    {"id": "clean_010", "score": 1.0,  "note": "Perfect"},
]


# ── Template data builders ────────────────────────────────────────────────────

def build_comparison_rows() -> list:
    """Build rows untuk model comparison table."""
    rows = [
        {
            "metric": "Avg Faithfulness",
            "mistral_val": "56.7%",
            "phi3_val": "46.7%",
            "mistral_wins": True,
            "tie": False,
        },
        {
            "metric": "Perfect Score Rate",
            "mistral_val": "5/10",
            "phi3_val": "4/10",
            "mistral_wins": True,
            "tie": False,
        },
        {
            "metric": "Failure Rate",
            "mistral_val": "50%",
            "phi3_val": "60%",
            "mistral_wins": True,   # lower is better
            "tie": False,
        },
        {
            "metric": "False INSUF_CTX",
            "mistral_val": "2",
            "phi3_val": "2",
            "mistral_wins": False,
            "tie": True,
        },
        {
            "metric": "Avg Latency / case",
            "mistral_val": "463.7s (7.7 min)",
            "phi3_val": "222.9s (3.7 min)",
            "mistral_wins": False,  # phi3 faster
            "tie": False,
        },
        {
            "metric": "Total Eval Time",
            "mistral_val": "160 min",
            "phi3_val": "77.7 min",
            "mistral_wins": False,
            "tie": False,
        },
    ]
    return rows


def build_findings() -> list:
    return [
        {
            "type": "danger",
            "title": "False INSUFFICIENT_CONTEXT",
            "body": (
                "Mistral returned <code>INSUFFICIENT_CONTEXT</code> for clean_001 and clean_004 "
                "despite explicit evidence in context. clean_004 is extreme — ground_truth to context "
                "similarity was 0.9691 yet model refused to answer. "
                "Framework detects this via two-stage validation: if similarity ≥ 0.65, "
                "evidence exists → score 0.0 (not 1.0)."
            ),
        },
        {
            "type": "warning",
            "title": "Claim Granularity — Enumeration Expansion",
            "body": (
                "Evaluating a sentence with 5 items as one claim produces false positives. "
                "<code>_expand_enumeration()</code> splits 'X includes A, B, C' into atomic claims. "
                "Score changed from 1.0 → 0.6 for the same answer after this fix."
            ),
        },
        {
            "type": "warning",
            "title": "Semantic ≠ Factual — PUT vs PATCH",
            "body": (
                "PUT vs PATCH yields cosine similarity 0.8337 — above the SUPPORTED threshold. "
                "Framework now applies technical conflict detection: if claim and evidence use "
                "different terms from the same group (HTTP methods, status codes), "
                "similarity is overridden to 0.0 regardless of embedding score."
            ),
        },
        {
            "type": "danger",
            "title": "Mistral is Faithful to Wrong Context",
            "body": (
                "Distractor dataset: faithfulness dropped from 56.7% → 8.3%. "
                "When context said <code>skip=5, limit=100</code>, model answered exactly that — "
                "ignoring that the correct values are skip=0, limit=10. "
                "Garbage in, garbage out. Retrieval quality fully determines answer quality."
            ),
        },
        {
            "type": "",
            "title": "Short Answer Blind Spot — Fixed in v1.2.1",
            "body": (
                "<code>'PUT'</code> and <code>'/files/...'</code> scored 0.0 because claim extractor "
                "produced 0 claims from single-token answers. Fixed: answers under 5 words use "
                "exact match path (exact → substring → char similarity) instead of semantic similarity."
            ),
        },
        {
            "type": "success",
            "title": "phi3:mini is 2.1x Faster with 10% Faithfulness Tradeoff",
            "body": (
                "Mistral: 56.7% avg faithfulness, 463.7s per case. "
                "phi3:mini: 46.7% avg faithfulness, 222.9s per case. "
                "phi3:mini over-generates (7 claims on clean_007, all below threshold) — "
                "verbose but not precise. For development iteration, phi3:mini is the right choice."
            ),
        },
    ]


def build_thresholds() -> list:
    return [
        {"name": "Faithfulness evidence matching",   "value": "0.75"},
        {"name": "Insufficient context validation",  "value": "0.65"},
        {"name": "Consistency semantic",             "value": "0.72"},
        {"name": "Consistency ROUGE-L",              "value": "0.40"},
        {"name": "Robustness sensitivity",           "value": "0.85"},
        {"name": "Short answer char similarity",     "value": "0.85"},
    ]


def build_limitations() -> list:
    return [
        {
            "title": "Single embedding model",
            "body": "All similarity calculations use nomic-embed-text. Calibrated thresholds may need adjustment for other embedding models.",
        },
        {
            "title": "Small dataset",
            "body": "10 clean cases, 5 distractor cases. Thresholds calibrated from limited observations — may need recalibration at scale.",
        },
        {
            "title": "Single domain",
            "body": "Only FastAPI documentation evaluated. Framework behavior on other technical domains not yet verified.",
        },
        {
            "title": "ROUGE-L sensitivity",
            "body": "Highly sensitive to stopwords and word order. Semantically identical answers with different structure will score low.",
        },
        {
            "title": "Short answer exact match not validated at scale",
            "body": "New implementation in v1.2.1 — threshold of 0.85 char similarity chosen empirically, needs more data to confirm.",
        },
    ]


def build_charts(embed: bool = False) -> list:
    """
    Build chart list untuk template.
    embed=True: chart di-encode jadi base64 (self-contained HTML)
    embed=False: chart pakai relative path (lebih kecil, butuh file PNG)
    """
    chart_defs = [
        {"file": "model_comparison.png",         "caption": "fig 1 — model comparison: 4 metrics side by side", "full": True},
        {"file": "per_case_comparison.png",       "caption": "fig 2 — per-case faithfulness: mistral vs phi3:mini", "full": True},
        {"file": "faithfulness_clean.png",        "caption": "fig 3 — mistral faithfulness per case, clean dataset", "full": False},
        {"file": "faithfulness_distractor.png",   "caption": "fig 4 — mistral faithfulness per case, distractor dataset", "full": False},
        {"file": "comparison_clean_vs_distractor.png", "caption": "fig 5 — clean vs distractor: outcome distribution", "full": False},
        {"file": "latency_comparison.png",        "caption": "fig 6 — inference latency: mistral vs phi3:mini", "full": False},
    ]

    charts = []
    for c in chart_defs:
        path = CHARTS_DIR / c["file"]
        if not path.exists():
            continue

        if embed:
            data = base64.b64encode(path.read_bytes()).decode()
            src = f"data:image/png;base64,{data}"
        else:
            # Path relatif dari lokasi output HTML
            src = f"charts/{c['file']}"

        charts.append({"path": src, "caption": c["caption"], "full": c["full"]})

    return charts


# ── Main render function ──────────────────────────────────────────────────────

def generate_report(output_path: str, embed_charts: bool = False) -> str:
    """Render HTML report dan simpan ke output_path. Return path yang disimpan."""

    # Load data dari JSON jika ada, fallback ke hardcoded
    mistral_clean_raw      = load_latest_faithfulness("mistral", "clean")
    mistral_distractor_raw = load_latest_faithfulness("mistral", "distractor")
    phi3_clean_raw         = load_latest_faithfulness("phi3:mini", "clean")
    mistral_summary        = load_latest_summary("mistral", "clean")

    # Per-case data
    def _from_json(raw: list, fallback: list) -> list:
        if not raw:
            return fallback
        return [
            {
                "id":       r["case_id"],
                "score":    r["faithfulness_score"],
                "question": r.get("question", "")[:80],
                "note":     r["failure_cases"][0]["diagnosis"][:100] if r.get("failure_cases") else "Pass",
            }
            for r in raw
        ]

    mistral_clean_cases      = _from_json(mistral_clean_raw, MISTRAL_CLEAN_DATA)
    mistral_distractor_cases = _from_json(mistral_distractor_raw, MISTRAL_DISTRACTOR_DATA)
    phi3_clean_cases         = _from_json(phi3_clean_raw, PHI3_CLEAN_DATA)

    # Aggregate stats
    m_scores = [c["score"] for c in mistral_clean_cases if c.get("score") is not None]
    mistral_avg = round(sum(m_scores) / len(m_scores), 3) if m_scores else 0.567

    avg_lat = mistral_summary.get("inference", {}).get("avg_latency_seconds", 463.7)

    template_data = {
        "title":             "Faithfulness & Consistency Evaluation",
        "evaluator_version": "1.2.1",
        "generated_at":      datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC"),
        "run_id":            mistral_summary.get("run_id", "20260221_074512"),
        "dataset_domain":    "FastAPI Documentation",
        "infrastructure":    "CPU-only · 16GB RAM · Ollama",

        "mistral_clean": {
            "avg_score":    f"{mistral_avg:.1%}",
            "perfect_count": sum(1 for s in m_scores if s == 1.0),
            "total":         len(m_scores),
            "failure_rate":  f"{sum(1 for c in mistral_clean_cases if c.get('score', 1) < 1.0) / max(len(mistral_clean_cases), 1):.0%}",
            "false_insuf":   "2",
            "avg_latency":   f"{avg_lat:.0f}",
        },

        "comparison_rows":       build_comparison_rows(),
        "mistral_clean_cases":   mistral_clean_cases,
        "mistral_distractor_cases": mistral_distractor_cases,
        "phi3_clean_cases":      phi3_clean_cases,
        "charts":                build_charts(embed=embed_charts),
        "findings":              build_findings(),
        "thresholds":            build_thresholds(),
        "limitations":           build_limitations(),
    }

    # Render
    env      = Environment(loader=FileSystemLoader(str(TEMPLATES_DIR)))
    template = env.get_template("report.html.j2")
    html     = template.render(**template_data)

    out = Path(output_path)
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text(html, encoding="utf-8")

    return str(out)


# ── CLI ───────────────────────────────────────────────────────────────────────

def parse_args():
    parser = argparse.ArgumentParser(description="LLM Eval — HTML Report Generator")
    parser.add_argument("--output", default="reports/report.html",
                        help="Output path (default: reports/report.html)")
    parser.add_argument("--embed-charts", action="store_true",
                        help="Embed charts as base64 — self-contained single file")
    return parser.parse_args()


if __name__ == "__main__":
    args   = parse_args()
    output = generate_report(args.output, embed_charts=args.embed_charts)
    print(f"Report generated: {output}")
    print(f"Open with: xdg-open {output}   (Linux)")
    print(f"           open {output}        (Mac)")